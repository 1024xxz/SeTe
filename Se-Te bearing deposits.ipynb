{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a22e81",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51342a6",
   "metadata": {},
   "source": [
    "## KNN Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(r'xxx.csv')\n",
    "\n",
    "# Separate columns of labels (type)\n",
    "labels = data['Type']\n",
    "data = data.drop(columns=['Type'])\n",
    "\n",
    "# Initialise the KNN filler\n",
    "imputer = KNNImputer(n_neighbors=5)  # You can adjust the n_neighbors parameter to set the K value\n",
    "\n",
    "# Use KNN to fill the vacancies",
    "data_filled = imputer.fit_transform(data)\n",
    "\n",
    "# Transform the filled data into a DataFrame\n",
    "data_filled = pd.DataFrame(data_filled, columns=data.columns)\n",
    "\n",
    "# Add the labelled columns back into the data\n",
    "data_filled['Type'] = labels\n",
    "\n",
    "# Save the filled data to a new CSV file",
    "data_filled.to_csv(r'xxx.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7107fa",
   "metadata": {},
   "source": [
    "## CLR Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(r'xxx.csv')\n",
    "\n",
    "# Separate columns of labels (type)\n",
    "labels = data['Type']\n",
    "data = data.drop(columns=['Type'])\n",
    "\n",
    "# Perform a centred logarithmic transformation on the data\n",
    "data_transformed = np.log1p(data)",
    "\n",
    "# Add the labelled columns back into the data\n",
    "data_transformed['Type'] = labels\n",
    "\n",
    "# Save the transformed data to a new CSV file\n",
    "data_transformed.to_csv(r'xxx.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88582bd",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c089cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "file_path = r'xxx.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Set the font to Palatino Linotype\n",
    "plt.rcParams['font.family'] = 'Palatino Linotype'\n",
    "\n",
    "# Use Seaborn to plot the scatterplot matrix\n",
    "pairplot = sns.pairplot(data, hue='Type', markers='o', palette='tab10')\n",
    "\n",
    "# Modify the legend's labels\n",
    "plt.legend(title='Type', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "current_legend = plt.gca().get_legend()\n",
    "\n",
    "# Modify the label characters of the legend\n",
    "new_labels = ['Skarn', 'VMS', 'Epithermal', 'Orogenic', 'Carlin', 'Porphyry', 'Magmatic Sulfide']\n",
    "for i, label in enumerate(current_legend.texts):\n",
    "    label.set_text(new_labels[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c791012",
   "metadata": {},
   "source": [
    "# Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assume data is loaded into a DataFrame named 'data'\n",
    "data = pd.read_csv(r'xxx.csv')\n",
    "\n",
    "# Remove 'Type' column\n",
    "data_without_type = data.drop('Type', axis=1)\n",
    "\n",
    "# Group by 'Type' and perform Spearman correlation analysis\n",
    "types = data['Type'].unique()\n",
    "\n",
    "for t in types:\n",
    "    subset = data[data['Type'] == t]\n",
    "    \n",
    "    # Drop 'Type' column and calculate Spearman correlation using listwise method\n",
    "    correlation_matrix = subset.drop('Type', axis=1).dropna().corr(method='spearman', min_periods=1)  \n",
    "    \n",
    "    # Use 'Blues' colormap for blue to white colors\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt=\".2f\", linewidths=.5, vmin=-1, vmax=1, annot_kws={\"size\": 10}, cbar=True)\n",
    "    plt.title(f\"Spearman Correlation Matrix - Type {t}\", fontsize=16)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# Perform Spearman correlation analysis for the entire dataset\n",
    "correlation_matrix_total = data_without_type.dropna().corr(method='spearman', min_periods=1)\n",
    "\n",
    "# Use 'Blues' colormap for blue to white colors\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix_total, annot=True, cmap='Blues', fmt=\".2f\", linewidths=.5, vmin=-1, vmax=1, annot_kws={\"size\": 10}, cbar=True)\n",
    "plt.title(\"Total Spearman Correlation Matrix\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6c4c9d",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7779ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(r'xxxx.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract the category type column\n",
    "categories = df.iloc[:, 0]\n",
    "\n",
    "# Extract the features column\n",
    "features = df.iloc[:, 1:]\n",
    "\n",
    "# Standardise feature columns\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Create pca model, choose number of principal components (2 principal components here)\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Get the contribution of each feature to the first two principal components\n",
    "feature_contributions = pca.components_\n",
    "\n",
    "# Compose a DataFrame of contributions and feature names\n",
    "df_feature_contributions = pd.DataFrame(data=feature_contributions.T,\n",
    "                                         columns=['PC1 Contribution', 'PC2 Contribution'],\n",
    "                                         index=features.columns)\n",
    "\n",
    "# Draw a bar chart\n",
    "df_feature_contributions.plot(kind='bar', figsize=(12, 8))\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Contribution to Principal Components')\n",
    "plt.title('Contribution of Features to Principal Components')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f885b66",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5f2c0",
   "metadata": {},
   "source": [
    "## Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# 1. Load CSV data\n",
    "data = pd.read_csv(r'xxx.csv')",
    "\n",
    "# 2. Segment the dataset into features (X) and labels (y)\n",
    "X = data.iloc[:, 1:]",
    "y = data.iloc[:, 0]",
    "\n",
    "# 3. Segment the dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4.Define parameter ranges\n",
    "param_grid = {'C': [0.01,0.1, 1, 10, 100,1000,10000,100000],\n",
    "              'gamma': [0.00001,0.0001,0.001, 0.01, 0.1, 1, 10]}\n",
    "# 5. Create SVM Classifier\n",
    "svm_classifier = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "# 6. Create GridSearchCV Objects\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "# 7. Perform Grid Search to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "# 8. Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# 9. heat map visualisation (with values)\n",
    "scores = grid_search.cv_results_[\"mean_test_score\"].reshape(len(param_grid['C']), len(param_grid['gamma']))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(scores, annot=True, fmt=\".3f\", cmap=plt.cm.cividis, cbar=True)\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"C\")\n",
    "plt.xticks(np.arange(len(param_grid['gamma'])) + 0.5, param_grid['gamma'], rotation=45, ha=\"right\")\n",
    "plt.yticks(np.arange(len(param_grid['C'])) + 0.5, param_grid['C'])\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c6efe-3e7c-44f5-9fb1-8f1969af5f1c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed246d22-78f2-4587-ac9e-4a48bbf0ebcc",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb9bcc-c62f-4447-859a-fc321f29e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Use the model with the best parameters for prediction\n",
    "best_svm_classifier = SVC(kernel='rbf', probability=True, random_state=42, **best_params)\n",
    "best_svm_classifier.fit(X_train, y_train)\n",
    "y_pred_svm = best_svm_classifier.predict(X_test)\n",
    "\n",
    "# 11. visualised confusion matrix\n",
    "confusion_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_svm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"SVM Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416b0c1-2912-47ec-9da3-95a403cf97eb",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed92f4-19d1-467f-b389-69342eaad259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "# Calculate precision, recall, F1 score for each category\n",
    "precision_svm = precision_score(y_test, y_pred_svm, average=None)\n",
    "recall_svm = recall_score(y_test, y_pred_svm, average=None)\n",
    "f1_svm = f1_score(y_test, y_pred_svm, average=None)\n",
    "\n",
    "# Print the precision, recall, and F1 score for each category\n",
    "for i in range(len(np.unique(y))):\n",
    "    print(f\"SVM - Type {i}:\")\n",
    "    print(f\"  Precision: {precision_svm[i]:.4f}\")\n",
    "    print(f\"  Recall: {recall_svm[i]:.4f}\")\n",
    "    print(f\"  F1 score: {f1_svm[i]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM - Accuracy: {accuracy_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d31ad0e",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337d0b9-4b61-40a3-86b2-bafc5bc29b96",
   "metadata": {},
   "source": [
    "## Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV data (replace with your actual CSV file path)\n",
    "data = pd.read_csv(r'D:\\LW\\DATA_NEW_CLR_COPY.csv')\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = data.iloc[:, 1:]  # Assuming all columns except the first one are features\n",
    "y = data.iloc[:, 0]   # The first column is the label\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation for each number of trees\n",
    "n_trees = list(range(1, 201, 10))\n",
    "cv_scores = []\n",
    "for n in n_trees:\n",
    "    rfc.n_estimators = n\n",
    "    scores = cross_val_score(rfc, X_train, y_train, cv=10, scoring='accuracy')  # Use 10-fold cross-validation\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Find the maximum cross-validated accuracy and corresponding number of trees\n",
    "max_cv_accuracy = max(cv_scores)\n",
    "optimal_n_estimators_cv = n_trees[cv_scores.index(max_cv_accuracy)]\n",
    "print(\"Optimal number of trees with k-fold cross-validation:\", optimal_n_estimators_cv)\n",
    "print(\"Maximum cross-validated accuracy:\", max_cv_accuracy)\n",
    "\n",
    "# Plot cross-validated accuracy against the number of trees\n",
    "plt.figure(figsize=[8, 6])\n",
    "plt.plot(n_trees, cv_scores)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.title('Random Forest Classifier Performance with k-fold Cross-Validation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5685d269",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff302bac-1d6d-4b53-9504-ab7ca52c287f",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2429182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest model with the optimal number of trees\n",
    "rfc_optimal = RandomForestClassifier(n_estimators=optimal_n_estimators_cv, n_jobs=-1, random_state=42)\n",
    "rfc_optimal.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rfc_optimal.predict(X_test)\n",
    "\n",
    "# Display confusion matrix\n",
    "confusion_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_rf, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caadec62-ee07-4597-b173-8c894a055d74",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602921a7-a741-4d95-92e5-4c466a2dd0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "# Calculate precision, recall, F1 score for each category\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average=None)\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average=None)\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average=None)\n",
    "\n",
    "# Print precision, recall, F1 score for each category\n",
    "for i in range(len(np.unique(y))):\n",
    "    print(f\"SVM - Type {i}:\")\n",
    "    print(f\"  Precision: {precision_rf[i]:.4f}\")\n",
    "    print(f\"  Recall: {recall_rf[i]:.4f}\")\n",
    "    print(f\"  F1 score: {f1_rf[i]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"RF - Accuracy: {accuracy_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a84fd",
   "metadata": {},
   "source": [
    "# Decision boundary (horizontal and vertical axes of a two-dimensional discriminant chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea96d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the SVM classifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA  \n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pdn",
    "df_wine=pd.read_csv(r'xxx.csv')\n",
    "mydata_data=df_wine[df_wine.columns[1:]].values\n",
    "mydata_target=df_wine['Type'].values\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "stdScale1 = StandardScaler().fit(mydata_data)   \n",
    "mydata_trainScaler = stdScale1.transform(mydata_data)  \n",
    "x1=mydata_trainScaler\n",
    "y1=mydata_target\n",
    "\n",
    "gamma=50\n",
    "svc=svm.SVC(kernel='rbf',C=10,gamma=gamma)\n",
    "svc.fit(x1,y1)\n",
    "print('SV number:',svc.support_)\n",
    "print('SV set:',svc.support_vectors_)\n",
    "print('SVC score:',svc.score(x1,y1))\n",
    "print(40*'*')\n",
    "logi = LogisticRegression(C=1.0,penalty='l2',solver='sag',max_iter=1000)\n",
    " \n",
    "svc_linear=svm.SVC(C=1.0,kernel=\"linear\")\n",
    " \n",
    "svc_rbf1=svm.SVC(C=1.0,kernel=\"rbf\",gamma=0.5)\n",
    " \n",
    "svc_rbf2=svm.SVC(C=1.0,kernel=\"rbf\",gamma=50)\n",
    "clfs=[logi,svc_linear,svc_rbf1,svc_rbf2]\n",
    "titles=[\"Logistic Steele regression\",'Linear regression function SVM','RBF kernel function (gamma=0.5)','RBF kernel function (gamma=50)']\n",
    "clr1=[logi]\n",
    " \n",
    "for clf,i in zip(clfs,range(len(clfs))):\n",
    "    clf.fit(x1,y1)\n",
    "    print(titles[i],'Performance scores on the full sample set：',clf.score(x1,y1))\n",
    "print(40*'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d7332",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f821835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv(r'xxx.csv')",
    "\n",
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=175, max_depth=20, random_state=42)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "explainer = shap.TreeExplainer(rf_classifier)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test, feature_names=data.columns[1:], class_names=np.unique(y), plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b658104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Palatino Linotype']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 1. Load CSV data\n",
    "data = pd.read_csv(r'xxxx.csv')\n",
    "\n",
    "# 2. Split the dataset into features (X) and labels (y)\n",
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "# 3. Divide the dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Create a random forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=151, max_depth=20, random_state=42)\n",
    "\n",
    "# 5. train_classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 6. Use shap to calculate Shapley values\n",
    "explainer = shap.TreeExplainer(rf_classifier)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "for output_index in range(len(shap_values)):\n",
    "    shap.summary_plot(shap_values[output_index], X_test, plot_type=\"dot\", title=f\"Output {output_index}\", feature_names=X_test.columns)\n",
    "    plt.figure()",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
